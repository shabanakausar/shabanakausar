{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOP52RAdRHbmjV1Z6hivdJE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shabanakausar/shabanakausar/blob/main/Q_Learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w2U4RLgnhXuz",
        "outputId": "a5c839cc-01fd-4ea8-8527-16600a1bc964"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{0: 'L1', 1: 'L2', 2: 'L3', 3: 'L4', 4: 'L5', 5: 'L6', 6: 'L7', 7: 'L8', 8: 'L9', 9: 'L10', 10: 'L11', 11: 'L12'}\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "gamma = 0.85 #discount factor\n",
        "alpha = 0.9  # learning rate\n",
        "\n",
        "# define the states(Map each of the locations to numbers(states))\n",
        "location_to_state = {'L1' : 0,\n",
        "                     'L2' : 1,\n",
        "                     'L3' : 2,\n",
        "                     'L4' : 3,\n",
        "                     'L5' : 4,\n",
        "                     'L6' : 5,\n",
        "                     'L7' : 6,\n",
        "                     'L8' : 7,\n",
        "                     'L9' : 8,\n",
        "                     'L10': 9,\n",
        "                     'L11': 10,\n",
        "                     'L12': 11\n",
        "    }\n",
        "# define our action(actions here represents the transition to the next state:)\n",
        "actions = [0,1,2,3,4,5,6,7,8,9,10,11]\n",
        "#define the reward (get the reward table)\n",
        "rewards = np.array(\n",
        "            [[0,1,0,0,0,0,0,0,0,0,0,0],\n",
        "            [1,0,0,0,0,0,0,0,1,0,0,0],\n",
        "            [0,0,0,0,0,1,0,0,0,0,0,0],\n",
        "            [0,0,0,0,1,0,1,0,0,0,0,0],\n",
        "            [0,0,0,1,0,0,0,1,0,0,0,0],\n",
        "            [0,0,1,0,0,0,0,0,0,1,0,0],\n",
        "            [0,0,0,1,0,0,0,0,0,0,0,0],\n",
        "            [0,0,0,0,1,0,0,0,1,0,0,0],\n",
        "            [0,1,0,0,0,0,0,1,0,0,1,0],\n",
        "            [0,0,0,0,0,1,0,0,0,0,0,1],\n",
        "            [0,0,0,0,0,0,0,0,1,0,0,1],\n",
        "            [0,0,0,0,0,0,0,0,0,1,1,0]])\n",
        "\n",
        "# Now,lets inverse mapping from the states back to original location indicators.\n",
        "#since it will help identify locations using their current state\n",
        "state_to_location = dict((state,location) for location,state in\n",
        "location_to_state.items())\n",
        "\n",
        "print(state_to_location)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_optimal_route(starting_location,end_location):\n",
        "\n",
        "    # lets make a copy of the reward matrix, we dont want to alter the origianl\n",
        "    #table.\n",
        "\n",
        "        rewards_copy = np.copy(rewards )\n",
        "        print(\"rewards_copy is :\",\"\\n\", rewards_copy)\n",
        "        print()\n",
        "\n",
        "        # create a random location for the golden key\n",
        "        golden_key_location = np.random.randint(0,11)\n",
        "        list_location_to_state = list(location_to_state.keys())\n",
        "        end_location = list_location_to_state[golden_key_location]\n",
        "        print(end_location)\n",
        "\n",
        "        # Get the ending state corresponding to the ending location as given\n",
        "\n",
        "        ending_state = location_to_state[end_location]\n",
        "        print(ending_state)\n",
        "        print()\n",
        "\n",
        "        # Automatically set the priority of the\n",
        "        # given ending state to the highest one\n",
        "        rewards_copy[ending_state,ending_state]= 500\n",
        "        print(rewards_copy)\n",
        "\n",
        "        # Initialize Q -Values to zero\n",
        "        Q = np.array(np.zeros([12,12]))\n",
        "\n",
        "        # We will explore the environment for a while, by looping it through 1000\n",
        "        #times.\n",
        "        #We will then pick a state randomly from the set of states we defined above,\n",
        "        # lets call \"current_state\".\n",
        "        for i in range(1000):\n",
        "            # Pick up a state randomly\n",
        "            current_state = np.random.randint(0,12)\n",
        "\n",
        "            # iterating through the rewards matrix,we get the states that are\n",
        "            #directly reachable\n",
        "            # from the randomly chosen current state and we will then assign those\n",
        "            #states in a list\n",
        "            # called \"playable_actions\".\n",
        "            playable_actions = []\n",
        "            for j in range (12):\n",
        "                if rewards_copy[current_state,j]>0:\n",
        "                    playable_actions.append(j)\n",
        "\n",
        "                    # Pick an action randomly from the list of playable actions\n",
        "                    #leading us to the next state\n",
        "                    next_state = np.random.choice(playable_actions)\n",
        "                    # Compute the temporal difference\n",
        "                    # Note: The \"action\" here exactly refers to going to the \"next\n",
        "                    #state\"\n",
        "                    TD = rewards_copy[current_state,next_state ] + gamma*Q[next_state ,np.argmax(Q[next_state])]-Q[current_state,next_state]\n",
        "                    # Update the Q-Value using the Bellman equation\n",
        "                    Q[current_state,next_state]+= alpha*TD\n",
        "        # Initialize the optimal route with the starting location\n",
        "        route = [starting_location]\n",
        "        # we at this juncture will have to set the next_location to the\n",
        "        #starting_location, reason being that\n",
        "        # we dont know what move the agent will be making at this moment\n",
        "        next_location = starting_location\n",
        "\n",
        "        # A \"while loop will be goog for iteration in this case\",\n",
        "        # we don't know the exact number of iterations required to reach to the\n",
        "        #final location.\n",
        "        while(next_location != end_location):\n",
        "            # pickup the starting state\n",
        "            starting_state = location_to_state[starting_location]\n",
        "            # pickup the highest Q-value pertaining to starting state\n",
        "            next_state = np.argmax(Q[starting_state])\n",
        "            # get the letters of the locations corresponding to the next state\n",
        "            next_location = state_to_location[next_state]\n",
        "            route.append(next_location)\n",
        "            # Now we update the starting location for the next iteration\n",
        "            starting_location = next_location\n",
        "\n",
        "        return route\n",
        "\n",
        "print(get_optimal_route(\"L12\",\"L1\"))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L_bz7dLJhZ7J",
        "outputId": "e70bc126-845e-467b-d52a-a7ced83862ae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "rewards_copy is : \n",
            " [[0 1 0 0 0 0 0 0 0 0 0 0]\n",
            " [1 0 0 0 0 0 0 0 1 0 0 0]\n",
            " [0 0 0 0 0 1 0 0 0 0 0 0]\n",
            " [0 0 0 0 1 0 1 0 0 0 0 0]\n",
            " [0 0 0 1 0 0 0 1 0 0 0 0]\n",
            " [0 0 1 0 0 0 0 0 0 1 0 0]\n",
            " [0 0 0 1 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 1 0 0 0 1 0 0 0]\n",
            " [0 1 0 0 0 0 0 1 0 0 1 0]\n",
            " [0 0 0 0 0 1 0 0 0 0 0 1]\n",
            " [0 0 0 0 0 0 0 0 1 0 0 1]\n",
            " [0 0 0 0 0 0 0 0 0 1 1 0]]\n",
            "\n",
            "L6\n",
            "5\n",
            "\n",
            "[[  0   1   0   0   0   0   0   0   0   0   0   0]\n",
            " [  1   0   0   0   0   0   0   0   1   0   0   0]\n",
            " [  0   0   0   0   0   1   0   0   0   0   0   0]\n",
            " [  0   0   0   0   1   0   1   0   0   0   0   0]\n",
            " [  0   0   0   1   0   0   0   1   0   0   0   0]\n",
            " [  0   0   1   0   0 500   0   0   0   1   0   0]\n",
            " [  0   0   0   1   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   1   0   0   0   1   0   0   0]\n",
            " [  0   1   0   0   0   0   0   1   0   0   1   0]\n",
            " [  0   0   0   0   0   1   0   0   0   0   0   1]\n",
            " [  0   0   0   0   0   0   0   0   1   0   0   1]\n",
            " [  0   0   0   0   0   0   0   0   0   1   1   0]]\n",
            "['L12', 'L10', 'L6']\n"
          ]
        }
      ]
    }
  ]
}